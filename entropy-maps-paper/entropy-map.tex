% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Entropy Maps},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Entropy Maps}
\author{}
\date{\vspace{-2.5em}2021-06-17}

\begin{document}
\maketitle
\begin{abstract}
Entropy maps model the concept of Bernoulli maps, a probabilistic model
designed to underpin the understanding and implementation of
probabilistic data structures. Entropy maps are presented as a novel
methodology for enabling oblivious functions through hashing mechanisms
that map domain values to codomain values with prefix-free codes, thus
facilitating operations without revealing plaintext data. By abstracting
the probabilistic structure of Bernoulli maps, this study highlights
their utility in propagating error probabilities and managing set
membership queries in a manner that preserves privacy and security.
\end{abstract}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{entropy-maps}{%
\section{Entropy Maps}\label{entropy-maps}}

The basic theory behind an entropy map is to map values in the domain to
values in the codomain by \emph{hashing} to a prefix-free code in the
codomain. We do not store anything related to the domain, since we are
simply hashing them, and a prefix of that hash will be used as a code
for a value in the codomain.

We actually allow for many different codes for each value in the
codomain, so that, for instance, a code for, say, the value \texttt{a}
may be \texttt{00}, \texttt{01}, \texttt{10}, and \texttt{11}. Notice
that we can efficiently decode this as \texttt{a} if the hash is less
than 4.

Suppose \(\Pr\{f(X) = y\} = p_y\), \(X \sim p_X\), then the optimally
space-efficient code, assuming a uniform hash function \(h\), is to
assign prefix-free codes for \(y\) whose probability of being mapped to
by \(h\) sums to \(p_y\). In this case, the expected bit length is given
by \[
    \ell = -\sum_{y \in \mathcal{Y}} p_y \log_2 p_y,
\] which if we imagine sampling \(x\) from \(\mathcal{X}\) with \(p_X\)
and then mapping to \(y = f(x)\) and observing the sequence of \(y\)'s,
then the expected bit length is the entropy of the sequence of \(y\)'s.
This is why we call it an entroy map.

If \(\mathcal{X}\) is finite, then we can just imagine implicitly
encoding the domain and then for each value in the domain, storing the
prefix-free code that it maps to, which has an average bit length of
\(\ell\)\$ and a total bit length of \(|X| \ell\).

\hypertarget{rate-distortion-bernoulli-maps}{%
\subsection{Rate distortion: Bernoulli
maps}\label{rate-distortion-bernoulli-maps}}

We can allow rate distortion, too, by failing to code for some of the
elements properly. For instance, a popular choice is when one of the
values, say \(y'\), is \emph{extremely} common such that, for instance,
\(p_{y'} > .99\), then we can give it a prefix-free code that sums to
\(p_{y'}\) and then \emph{not} code for it in the entropy map, in which
case it will, for some randomly selected \(x \in \mathcal{X}\), be
mapped to \(y'\) with probability \(p_{y'}\) (which is sufficiently
large, and can be made as close to \(1\) as desired if we wish to trade
space for accuracy), and then for the remaining values in the domain,
code for them correctly (or also allow errors on them, too, but only
after trying to find correct codes for each of them).

\hypertarget{bernoulli-set-indicator-function}{%
\subsubsection{Bernoulli set-indicator
function}\label{bernoulli-set-indicator-function}}

For instance, suppose we have a set-indicator function \[
    1_{\mathcal{A}} : \mathcal{X} \to \{0,1\},
\] where \(\mathcal{A} \subseteq \mathcal{X}\), and \(\mathcal{X}\) is a
very large set (even infinite), then we may assign prefix-free codes for
the codomain value \(1\) s.t. a priori, a random hash function hashes an
element in \(\mathcal{X}\) to a prefix-free code for \(1\) with
probability \(\varepsilon\), where \(\varepsilon\) is very small, e.g.,
\(2^{-10}\).

There are a countably infinite set of random hash functions which hash
all elements in \(\mathcal{A} \subseteq \mathcal{X}\) to prefix-free
codes for \(1\) and all other elements,
\(\mathcal{A}' = \mathcal{X} - \mathcal{A}\), to prefix codes either for
\(0\) or \(1\). If we are choosing a random hash function that satisfies
this property, then it is expected that \(\varepsilon\) of the elements
in \(\mathcal{A}'\) will hash to a prefix-free code for \(1\), and the
remaining \(1 - \varepsilon\) will hash to a prefix-free code for \(0\).

For any \(x \in \mathcal{X}\), we can test if \(1_{\mathcal{A}}(x) = 1\)
by testing if a prefix of \(h(x)\) is a prefix-free code for \(0\) or
\(1\), and if it is a code for \(0\), then we know that it is definitely
not a member of \(\mathcal{A}\), but if it is a code for \(1\), then it
is a member of \(\mathcal{A}\) with a false positive rate of
\(\varepsilon\) and a true positive rate \(1\), since a randomly drawn
element in \(\mathcal{A}'\) will hash to \(0\) with probability
\(1 - \varepsilon\) and any element in \(\mathcal{A}\) will map to \(1\)
with probability \(1\) (since we explicitly chose a random hash function
that hashes all of the elements in \(\mathcal{A}\) to a prefix-free code
for \(1\)).

It is interesting to note that the entropy map initially frames the
problem as a compression problem, but we can also think of it as a
rate-distortion problem. Implicitly, in the above set-indicator function
approximation, we are choosing to minimize a loss function in which
false negatives are much more costly than false negatives, either
because it is unlikely we will test a negative element for membership,
or because false positives are not nearly as costly as false negatives,
e.g., falsely thinking a rustling in the bushes is a tiger (false
positive) is much less costly than failing to notice a tiger in the
bushes (false negative).

In either case, we call this set-indicator approximation a Bernoulli
set-indicator function,
\texttt{bernoulli\textless{}(set\textless{}X\textgreater{},\ X)\ -\textgreater{}\ bool\textgreater{}\{}
\(1_A\) \texttt{\}}. This is the function that is communicated, not the
latent set-indicator function \(1_A\).

A randomly chosen random hash function that satisfies (is conditioned
on) the property that it hashes all elements in \(\mathcal{A}\) to a
prefix-free code for \(1\) has the confusion matrix in Table 1.

Table 1: Conditional distribution of Bernoulli set-indicator functions
given latent set-indicator function on \(\mathcal{X} = \{a,b\}\)

\begin{longtable}[]{@{}lllll@{}}
\toprule
\begin{minipage}[b]{0.13\columnwidth}\raggedright
latent/observed\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedright
\(1_\emptyset\)\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
\(1_{\{a\}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
\(1_{\{b\}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright
\(1_{\{a,b\}}\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(1_\emptyset\)\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
\((1-\varepsilon)^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\((1-\varepsilon)\varepsilon\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\((1-\varepsilon)\varepsilon\)\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\(\varepsilon^2\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(1_{\{a\}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\(1-\varepsilon\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\(\varepsilon\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(1_{\{b\}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\(1-\varepsilon\)\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\(\varepsilon\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright
\(1_{\{a,b\}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
\(1\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

We see that the constraint of no false negatives generates a confusion
matrix with a lot of zeros. If we observe
\texttt{bernoulli\textless{}set\textless{}X\textgreater{},X)\ -\textgreater{}\ bool\textgreater{}\{}\(1_{\{a\}}\)\texttt{\}},
then the latent set-indicator function is either \(1_{\emptyset}\) or
\(1_{\{a\}}\). Since \(\varepsilon\) is very small, we can be fairly
certain that the latent set-indicator function is \(1_{\{a\}}\).

What is the total degrees-of-freedom for a confusion matrix of this
type?

Table 2: Confusion matrix with maximum degrees-of-freedom

\begin{longtable}[]{@{}lllll@{}}
\toprule
latent/observed & \(1_\emptyset\) & \(1_{\{a\}}\) & \(1_{\{b\}}\) &
\(1_{\{a,b\}}\)\tabularnewline
\midrule
\endhead
\(1_\emptyset\) & \(p_{1 1}\) & \(p_{1 2}\) & \(p_{1 3}\) &
\(1-p_{1 1}-p_{1 2}-p_{1 3}\)\tabularnewline
\(1_{\{a\}}\) & \(p_{2 1}\) & \(p_{2 2}\) & \(p_{2 3}\) &
\(1-p_{2 1}-p_{2 2}-p_{2 3}\)\tabularnewline
\(1_{\{b\}}\) & \(p_{3 1}\) & \(p_{3 2}\) & \(p_{3 3}\) &
\(1-p_{3 1}-p_{3 2}-p_{3 3}\)\tabularnewline
\(1_{\{a,b\}}\) & \(p_{4 1}\) & \(p_{4 2}\) & \(p_{4 3}\) &
\(1-p_{4 1}-p_{4 2}-p_{4 3}\)\tabularnewline
\bottomrule
\end{longtable}

We see that there are \(4 \times (4 - 1) = 12\) degrees-of-freedom for
the confusion matrix in Table 2. For the confusion matrix in Table 1, we
have \(1\) degrees-of-freedom, since we have \(1\) parameter,
\(\varepsilon\).

The degree-of-freedom is one way to think about the complexity of a
model. The more degrees-of-freedom, the more complex the model. The more
complex the model, the more data we need to estimate the parameters of
the model, although frequently we already know the parameters of the
model, since it may have been specified as a part of the algrorithm that
generated the Bernoulli approximation.

The confusion matrix in Tables 1 and 2 represent the conditional
distribution of the Bernoulli set-indicator function given the latent
set-indicator function, which we denote by
\texttt{bernoulli\textless{}set\textless{}X\textgreater{},X)\ -\textgreater{}\ bool\textgreater{}}.

\hypertarget{boolean-bernoulli-as-constant-function}{%
\subsubsection{Boolean Bernoulli as constant
function}\label{boolean-bernoulli-as-constant-function}}

How many functions are there of type \texttt{()\ -\textgreater{}\ bool}?
There are two, \texttt{true} and \texttt{false}. That is, there are
\(|\{true, false\}|^{|\{1\}|} = 2^1 = 2\) functions.

So, we can also think of Boolean values as functions of type
\texttt{()\ -\textgreater{}\ bool}. Then, when we apply the Bernoulli
model
\texttt{bernoulli\textless{}()\ -\textgreater{}\ bool\textgreater{}}, we
get the same result as before.

Table 3: Confusion matrix for Bernoulli model applied to Boolean values

\begin{longtable}[]{@{}lll@{}}
\toprule
latent/observed & \texttt{true} & \texttt{false}\tabularnewline
\midrule
\endhead
\texttt{true} & \(p_{1 1}\) & \(1-p_{1 1}\)\tabularnewline
\texttt{false} & \(1-p_{2 2}\) & \(p_{2 2}\)\tabularnewline
\bottomrule
\end{longtable}

This confusion matrix has a maximum of two degrees-of-freedom, since
there are two parameters, \(p_{1 1}\) and \(p_{2 2}\), since we have the
constraint that the sum of the probabilities in each row is \(1\).

In the binary symmetric channel, \(p_{1 1} = p_{2 2}\):

Table 4: Confusion matrix for Bernoulli model applied to Boolean values

\begin{longtable}[]{@{}lll@{}}
\toprule
latent/observed & \texttt{true} & \texttt{false}\tabularnewline
\midrule
\endhead
\texttt{true} & \(p\) & \(1-p\)\tabularnewline
\texttt{false} & \(1-p\) & \(p\)\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{conditional-distribution-of-latent-function-given-observed-function}{%
\subsubsection{Conditional distribution of latent function given
observed
function}\label{conditional-distribution-of-latent-function-given-observed-function}}

Once we \emph{have} an observation, say
\texttt{bernoulli\textless{}set\textless{}X\textgreater{},X)\ -\textgreater{}\ bool\textgreater{}\{x\}},
what does the confusion matrix tell us? Let's abstract the problem a bit
so we can focus on deriving the result.

Let \(X\) and \(Y\) be random variables. Assume that
\(P(X = x | Y = y)\) is difficult to compute, but \(P(Y = y | X = x)\)
is easy. (This is the case for the confusion matrix. We know the
conditional distribution of the observed set-indicator function given
the latent set-indicator function, but we want to know the conditional
distribution of the latent set-indicator function given the observed
set-indicator function, which is not directly available.)

Suppose we are interested in \(P(X = x | Y = y)\) but we only know
\(P(Y = y | X = x)\). Then, we can use Bayes' rule to compute
\(P(X = x | Y = y)\): \[
P(X = x | Y = y) = \frac{P(Y = y | X = x) P(X = x)}{P(Y = y)}
\]

So, to compute \(P(X = x | Y = y)\), we need to know two additional
things. First, what is \(P(X = x)\)? This is usually a prior. If we know
something about the distribution of \(X\), then encode that information
in \(P(X = x)\), otherwise we can use an uninformed prior, e.g., assign
a uniform probability to each possibility.

Second, what is \(P(Y = y)\)? This is just a normalizing constant that
makes the conditional distribution of \(X\) given \(Y\) sum to \(1\),
but it can be computed as follows: \[
P(Y = y) = \sum_{x'} P(Y = y | X = x') P(X = x')
\] If \(P(X = x)\) is a uniform prior and \(|X|\) is finite then
\(P(X = x) = 1/|X|\). Combining this with the above equation, we get: \[
P(X = x | Y = y) = \frac{P(Y = y | X = x)}{\sum_{x'} P(Y = y | X = x')}
\]

So, let's replace \(X\) with the latent set-indicator function
\texttt{x} and \(Y\) with the observed
\texttt{bernoulli\textless{}(set\textless{}X\textgreater{},X\textgreater{}\ -\textgreater{}\ bool\textgreater{}\{y\}}.
Then, we can compute the conditional distribution of the latent
\texttt{x} given the observed
\texttt{bernoulli\textless{}(set\textless{}X\textgreater{},X\textgreater{}\ -\textgreater{}\ bool\textgreater{}\{y\}}
by looking at the confusion matrix in Table 2 and picking out the
specific row and column of interest and then normalizing by the sum of
the column.

For instance, suppose we observe \(1_{\{a\}}\). Then, we want to know
the conditional probability of the latent set-indicator function given
the observed set-indicator. The column for \(1_{\{a\}}\) is
\((p_{1 2}, p_{2 2}, p_{3 2}, p_{4 2})'\). The sum of this column is
\(p_{1 2} + p_{2 2} + p_{3 2} + p_{4 2} = 1\). So, the conditional
probability of the latent set-indicator function given the observed
set-indicator is given by \[
p_{k|2} = \frac{p_{k 2}}{\sum_{j=1}^4 p_{j 2}},
\] where \(k\) is the row corresponding to the latent set-indicator
function of interest and we conditioning on column \(2\), the index for
the observed set-indicator function \(1_{\{a\}}\).

More generally, we have \[
p_{k|i} = \frac{p_{k i}}{\sum_{j=1}^4 p_{j i}},
\] where \(k\) is the row corresponding to the latent set-indicator
function of interest and we are conditioning on column \(i\), the index
for the observed set-indicator function. If we do this for the four
possible observed set-indicator functions for Table 2 (confusion matrix
with only one degree-of-freedom), we get Table 5.

Table 5: Conditional probability of latent set-indicator function given
observed set-indicator function

\begin{longtable}[]{@{}lllll@{}}
\toprule
\begin{minipage}[b]{0.10\columnwidth}\raggedright
observed/latent\strut
\end{minipage} & \begin{minipage}[b]{0.21\columnwidth}\raggedright
\(1_\emptyset\)\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
\(1_{\{a\}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
\(1_{\{b\}}\)\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
\(1_{\{a,b\}}\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.10\columnwidth}\raggedright
\(1_\emptyset\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright
\(1\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\(0\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright
\(1_{\{a\}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright
\(\varepsilon/(1+\varepsilon)\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(1/(1+\varepsilon)\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\(0\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright
\(1_{\{b\}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright
\(\varepsilon/(1+\varepsilon)\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(0\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(1/(1+\varepsilon)\)\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\(0\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.10\columnwidth}\raggedright
\(1_{\{a,b\}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.21\columnwidth}\raggedright
\(\varepsilon^2/(1+\varepsilon)^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(\varepsilon/(1+\varepsilon)^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
\(\varepsilon/(1+\varepsilon)^2\)\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\(1/(1+\varepsilon)^2\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

The conditional distribution in Table 5 is one way to think about the
uncertainty of the latent set-indicator function given the observed
set-indicator function. The entropy is another way to think about the
uncertainty, but we will not compute it here.

We see that when we observe the empty set for a Bernoulli model in which
false negatives are not possible, then we know for certain that the
latent set-indicator function is \(1_\emptyset\). However, when we
observe \(1_{\{a\}}\), we are uncertain about the latent set-indicator
function. We know that it is either \(1_{\emptyset}\) or \(1_{\{a\}}\),
but we do not know which one it is. Since \(\varepsilon\) is small, it
is more much likely to be \(1_{\{a\}}\) than \(1_{\emptyset}\), though.
A similar argument holds for the other two observed set-indicator
functions.

\hypertarget{algorithms}{%
\subsection{Algorithms}\label{algorithms}}

The simplest algorithm is a one-level hash function evaluation, where we
hash the domain values concatenated with some bit string \(b\) such that
when we decode the values \(h(x + b)\), \(x \in \mathcal{X}\), we get a
prefix-free code for \(y = f(x)\).

\hypertarget{two-level-hash-function-evaluation}{%
\subsubsection{Two-level hash function
evaluation}\label{two-level-hash-function-evaluation}}

The more practical solution is a two-level hash scheme. First, we hash
each \(x \in \mathcal{X}\) concatented with the same bit string \(b\),
same as before. However, we use this hash value to index into a hash
table \(H\) at, say, index \(j\). Now, we choose a bit string for
\(H[j]\) for each \(x \in \mathcal{X}\) that hashes to \(j\) such that
\(f(x) = \text{decode}(h(x + H[j]))\).

This way, we can keep the probability
\(p_j = \prod_x \Pr\{ f(x) = \text{decode}(h(x + H[j]))\}\) for each
\(x\) that hashes to \(j\) more or less constant, independent of the
size of the codomain \(\mathcal{X}\), by choosing an appropriately-sized
hash table \(H\).

Since each decoding is an independent Bernoulli trial, we see that the
probability that a particular \(x\) that hashes to \(j\) is decoded
correctly is the number of hashes that are a prefix-free code for
\(f(x)\) divided by the total number of hashes (e.g., an \(N\) bit hash
function has \(2^N\) possible values).

\hypertarget{oblivious-entropy-maps}{%
\subsubsection{Oblivious entropy maps}\label{oblivious-entropy-maps}}

An \emph{oblivious} entropy map is just an entropy map where the hash
function is applied to trapdoors of \(\mathcal{X}\) and the prefix-free
codes for \(\mathcal{Y}\) have no rythm or reason to them, e.g., a
random selection of hash values for each value in \(\mathcal{Y}\).

\end{document}
