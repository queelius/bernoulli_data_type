The Boolean type, represented as {\ttfamily bool} in C++, models the set of values given by {\ttfamily \{true,false\}}. This document entertains the replacement of {\ttfamily bool} with a type {\ttfamily bernoulli$<$bool$>$}, which represents a sort of {\itshape noisy} Boolean. In general, we can have a Bernoulli type for any type {\ttfamily T}, denoed by {\ttfamily bernoulli$<$T$>$}.

Each Bernoulli Model also has an {\itshape order}, an integer greater than 1, and it essentially describes the number of independent ways in which the process that generated the Bernoulli approximation can produce errors. We denote that a Bernoulli Model has order {\ttfamily K} with {\ttfamily bernoulli$<$T,K$>$}. Unless it is useful, we drop the order information and simply write {\ttfamily bernoulli$<$T$>$}.

\begin{quote}
As special case, data structures like Bloom filters can be thought of as a Bernoulli data structure. \end{quote}
In the Bernoulli Boolean model, a {\ttfamily bool} is wrapped inside of a Bernoulli type {\ttfamily bernoulli$<$bool$>$}. We use the notation {\ttfamily bernoulli$<$bool$>$\{x\}} to denote that it is modeling some {\itshape latent} variable {\ttfamily x} (unobservable). We can think of {\ttfamily bernoulli$<$bool$>$\{x\}} as a measurement of {\ttfamily x}, or a noisy version of the original {\ttfamily x}, and it may or may not equal {\ttfamily x}.

The Bernoulli model introduces a notion of uncertainty or error. Specifically, a {\ttfamily bernoulli$<$bool$>$\{x\}} is a {\itshape random Bernoulli variable} such that 
\begin{DoxyCode}{0}
\DoxyCodeLine{Pr\{bernoulli<bool>\{x\} == x\} == p(x)}

\end{DoxyCode}
 where {\ttfamily 0 $<$ p(x) $<$ 1} is the probability of being correct and {\ttfamily 1-\/p(x)} is the probability of an error. In most practical situations, the probability {\ttfamily p(x)} is known and can be adjusted to balance factors like space and accuracy.\hypertarget{md_BERNOULLI_BOOL_autotoc_md1}{}\doxysection{Motivation}\label{md_BERNOULLI_BOOL_autotoc_md1}
A big reason for developing the Bernoulli Model formalism is so that we can use Bernoulli Models of data types to develop Oblivious Data Types. We will go into that in a separate document, but the basic idea is that Bernoulli approximations have a lot of desirable properties for developing oblivious data types, and the Bernoulli Model formalism allows us to reason about the correctness of the oblivious data types and to make them more space-\/efficient by trading accuracy for space while allowing for {\ttfamily O(1)} time complexity.

The Bernoulli Model also provides a formalism for how to think about various probabilistic data structures, like the Bloom filter, Count-\/\+Min sketch, or my invention, the Bernoulli data type, which comprises an entire family of data structures that are all based on the Bernoulli Model, from sets (like the Bloom filter) to maps in a near-\/space optimal way, while allowing for more savings by trading accuracy for space in a controlled way.

In this paper, we narrow our focus to the Boolean Bernoulli Model, which is the simplest Bernoulli Model. Later in this document, we consider Bernoulli Models for Boolean functions too, since it provides a natural opportunity to think about the model in a more general way.\hypertarget{md_BERNOULLI_BOOL_autotoc_md2}{}\doxysection{Binary Channels}\label{md_BERNOULLI_BOOL_autotoc_md2}
Let\textquotesingle{}s begin by thinking about the Binary Symmetric Channel and the Binary Asymmetric Channel. The Bernoulli Boolean model can exhibit two distinct behaviors, represented as different \char`\"{}channels\char`\"{} through which Boolean values are transmitted\+:


\begin{DoxyEnumerate}
\item {\bfseries{Binary Symmetric Channel (First-\/order Bernoulli model)}}\+: The probability of an equality error is the same for {\ttfamily true} and {\ttfamily false}. We denote this by the type {\ttfamily bernoulli$<$bool,1$>$}.
\item {\bfseries{Binary Asymmetric Channel (Second-\/order Bernoulli model)}}\+: The probability of an equality error differs for {\ttfamily true} and {\ttfamily false}. We denote this by the type {\ttfamily bernoulli$<$bool,2$>$}.
\end{DoxyEnumerate}\hypertarget{md_BERNOULLI_BOOL_autotoc_md3}{}\doxysection{False Positives and Negatives}\label{md_BERNOULLI_BOOL_autotoc_md3}
Errors in the Bernoulli Boolean model can be understood in terms of {\itshape false negatives} and {\itshape false positives}\+:


\begin{DoxyEnumerate}
\item {\ttfamily bernoulli$<$bool$>$\{false\} == true} is a {\itshape false negative}.
\item {\ttfamily bernoulli$<$bool$>$\{true\} == false} is a {\itshape false positive}.
\end{DoxyEnumerate}

In the first-\/order model, the probability of a false negative equals the probability of a false positive. In the second-\/order model, these probabilities differ. In a specific but common version of the second-\/order Bernoulli Boolean model, false negatives occur with probability 0 and false positives occur with probability {\ttfamily 0 $<$ \textbackslash{}varepsilon $<$ 1}.\hypertarget{md_BERNOULLI_BOOL_autotoc_md4}{}\doxysection{Prediction}\label{md_BERNOULLI_BOOL_autotoc_md4}
{\ttfamily bernoulli$<$bool$>$\{x\}} is {\itshape correlated} with {\ttfamily x}, and ideally, {\ttfamily bernoulli$<$bool$>$\{x\}} provides evidence for {\ttfamily x}, i.\+e., allows one to predict {\ttfamily x} given {\ttfamily bernoulli$<$bool$>$\{x\}} better than if no observations where given whatsoever. If the probability of correct {\ttfamily p(x)} is {\ttfamily $<$= 0.\+5} and we have no prior information about {\ttfamily x}, the best (ML) estimate of {\ttfamily x} is the observation {\ttfamily bernoulli$<$bool$>$\{x\}}.

However, with prior information about {\ttfamily x}, we can estimate the probability that the latent variable {\ttfamily x} is {\ttfamily true} or {\ttfamily false}. Using Bayes\textquotesingle{} rule, the probability that {\ttfamily bernoulli$<$bool$>$\{x\}} is correct is\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{Pr\{x == \textcolor{keyword}{true} | bernoulli<bool>\{x\} == \textcolor{keyword}{true}\} ==}
\DoxyCodeLine{    Pr\{bernoulli<bool>\{x\} == \textcolor{keyword}{true} | x == \textcolor{keyword}{true} \} * Pr\{x == \textcolor{keyword}{true}\}}
\DoxyCodeLine{    /}
\DoxyCodeLine{    (Pr\{bernoulli<bool>\{x\} == \textcolor{keyword}{true} | x == \textcolor{keyword}{true}\} * Pr\{x == \textcolor{keyword}{true}\} +}
\DoxyCodeLine{    Pr\{bernoulli<bool>\{x\} == \textcolor{keyword}{true} | x == \textcolor{keyword}{false}\} * (1-\/Pr\{x == \textcolor{keyword}{true}\}))}

\end{DoxyCode}


In the first-\/order model, if the probability of being correct {\ttfamily q}, then\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{Pr\{x == \textcolor{keyword}{true} | bernoulli<bool,1>\{x\} == \textcolor{keyword}{true}\} ==}
\DoxyCodeLine{    q * Pr\{x == \textcolor{keyword}{true}\}}
\DoxyCodeLine{    /}
\DoxyCodeLine{    (q * Pr\{x == \textcolor{keyword}{true}\} + (1-\/q) * (1-\/Pr\{x == \textcolor{keyword}{true}\}))}

\end{DoxyCode}


Assuming maximum ignorance (maximum entropy) about {\ttfamily x} (i.\+e., {\ttfamily Pr\{x == true\} == 0.\+5}), the following expression is obtained\+: 
\begin{DoxyCode}{0}
\DoxyCodeLine{Pr\{x == \textcolor{keyword}{true} | bernoulli<bool,1>\{x\} == \textcolor{keyword}{true}\} == q}

\end{DoxyCode}


One could even imagine having multiple sources of, say, noisy i.\+i.\+d. measurements of the same {\ttfamily x}. For instance, suppose {\ttfamily x == true} but we don\textquotesingle{}t know that and we have {\ttfamily 3} measurements of {\ttfamily x}. 
\begin{DoxyCode}{0}
\DoxyCodeLine{y1 = bernoulli<bool,1>\{\textcolor{keyword}{true}\} == \textcolor{keyword}{true}}
\DoxyCodeLine{y2 = bernoulli<bool,1>\{\textcolor{keyword}{true}\} == \textcolor{keyword}{false}}
\DoxyCodeLine{y3 = bernoulli<bool,1>\{\textcolor{keyword}{true}\} == \textcolor{keyword}{true}}

\end{DoxyCode}


This is more information about {\ttfamily x} than just one noisy observation. Clearly, and informally, the best prediction for the value of {\ttfamily x} is the majority vote, which is {\ttfamily true} in this case.

Consider this. The number of {\ttfamily true} values is Binomially distributed with parameters {\ttfamily n=3} (independent trials) and probability {\ttfamily p}, so we let {\ttfamily N $\sim$ BIN(3,p)} denote the random variable representing the number of {\ttfamily true} values in {\ttfamily y1, y2, y3}.

Let\textquotesingle{}s do a case by case analysis to compute the probability that the above majority vote is correct. First, for the majority vote to be correct, {\ttfamily N $>$= 2}, which means that {\ttfamily N == 2} or {\ttfamily N == 3}.


\begin{DoxyEnumerate}
\item The probability that {\ttfamily N == 2} is {\ttfamily Pr\{N == 2\} = 3 $\ast$ p$^\wedge$2 $\ast$ (1-\/p)}.
\item The probability that {\ttfamily N == 3} is {\ttfamily Pr\{N == 3\} = p$^\wedge$3}.
\end{DoxyEnumerate}

Therefore, the probability of no error is {\ttfamily 3 $\ast$ p$^\wedge$2 $\ast$ (1-\/p) + p$^\wedge$3}. If {\ttfamily p = 0.\+5} (maximum ignorance), we get a no error rate of {\ttfamily 0.\+5}, as intuitively expected. For {\ttfamily p = 1}, we get a no error rate of {\ttfamily 1}, which is also intuitively expected. The no error rate of a single observation, of course, is just {\ttfamily p}. Let\textquotesingle{}s plot these two no error rates together\+:



We see a slight improvement in the no error rate when we have multiple noisy observations of the same latent variable. As the number of independent sources goes to infinity, the error rate goes to 0.

This is not a typical use-\/case for the Bernoulli Boolean model, since it will mostly be a analytical result of probabilistic data structures that may be framed in the context of a Bernoulli model, but it is interesting to see how the model behaves in this case.\hypertarget{md_BERNOULLI_BOOL_autotoc_md5}{}\doxysection{Inducing Bernoulli types}\label{md_BERNOULLI_BOOL_autotoc_md5}
If we have a function {\ttfamily f \+: bool -\/$>$ bool}, then the space of all possible functions is given by Table 1.

Table 1\+: All possible functions {\ttfamily f \+: bool -\/$>$ bool}

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{3}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ f   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ f(true)   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ f(false)    }\\\cline{1-3}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ f   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ f(true)   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ f(false)    }\\\cline{1-3}
\endhead
id   &true   &false    \\\cline{1-3}
not   &false   &true    \\\cline{1-3}
true   &true   &true    \\\cline{1-3}
false   &false   &false   \\\cline{1-3}
\end{longtabu}


It may be interesting to consider what happens when we replace the Boolean inputs with Bernoulli boolean values and ask the question, \char`\"{}\+What is the probability that \`{}f(bernoulli$<$bool,1$>$\{x\}) == f(x)\`{}?\char`\"{}

Notice that {\ttfamily f(bernoulli$<$bool,1$>$\{x\})} is {\ttfamily f(x)} with some probability, but {\ttfamily f(x)} may be latent depending on {\ttfamily f}. For the constant fuctions, {\ttfamily true} and {\ttfamily false}, we get the same function, i.\+e., {\ttfamily true(bernoulli$<$bool,1$>$\{true\}) == true} since {\ttfamily true \+: bool -\/$>$ bool} always outputs {\ttfamily true}, and similiarly for {\ttfamily false \+: bool -\/$>$ bool}.

However, the {\ttfamily id} and {\ttfamily not} functions are different. For instance, suppose {\ttfamily Pr\{bernoulli$<$bool,1$>$\{x\} == x\} == p}. Then, when we input {\ttfamily bernoulli$<$bool,1$>$\{true\}} into {\ttfamily id}, we get the correct output {\ttfamily true} with probability {\ttfamily p} and the incorrect output {\ttfamily false} with probability {\ttfamily 1-\/p}. Likewise, when we input {\ttfamily bernoulli$<$bool,1$>$\{false\}} into {\ttfamily id}, we get the correct output {\ttfamily id(true) == false} with probability {\ttfamily p} and the incorrect output {\ttfamily f(false) == true} with probability {\ttfamily 1-\/p}, and a similar story for {\ttfamily not}.

Since we can think of these outputs as either correct or incorrect with probability {\ttfamily p}, we can call them Bernoulli Boolean values too, e.\+g., this is a function of type 
\begin{DoxyCode}{0}
\DoxyCodeLine{bernoulli<bool,1> -\/> bernoulli<bool,1>}

\end{DoxyCode}


What is this function? It\textquotesingle{}s just {\ttfamily id}, but it has been monadically lifted into the Bernoulli Boolean model. Notice also that this is distinct from the type 
\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordtype}{bool} -\/> bernoulli<bool,1>}

\end{DoxyCode}
 which is what we say is a Bernoulli map from {\ttfamily bool} to {\ttfamily bernoulli$<$bool,1$>$}. In this case, it is a first-\/order Bournoulli map on the equality of its output, i.\+e., 
\begin{DoxyCode}{0}
\DoxyCodeLine{Pr\{bernoulli<\textcolor{keywordtype}{bool} -\/> bool,1>\{\textcolor{keywordtype}{id}\}(x) == \textcolor{keywordtype}{id}(x)\} == p}

\end{DoxyCode}


Notice what the notation suggests, too. We are writing {\ttfamily bernoulli$<$bool -\/$>$ bool,1$>$\{id\}} to indicate that the true value is {\ttfamily id} but what we {\itshape observe} is {\ttfamily bernoulli$<$bool-\/$>$bool,1$>$\{id\}}. We cannot observe {\ttfamily id} directly. In fact, if we knew it was the identity function, we already know the correct output. We are interested in the case where we don\textquotesingle{}t know the correct output, and all we are given as evidence is the observation {\ttfamily bernoulli$<$bool-\/$>$bool,1$>$\{id\}}.

So, we are applying the {\ttfamily bernoulli} concept to the function type {\ttfamily bool -\/$>$ bool}, which in this case only has 4 possibilities. Clearly, we normally would {\itshape not} use a Bernoulli model for {\ttfamily bool -\/$>$ bool}, and rather, the Bernoulli model would be induced by some source of error, such as transmission over a noisy channel, as previously described. We stick to this simple example for now, though, because it is much more managable to work with, and we can generalize the results to {\ttfamily X -\/$>$ Y} where {\ttfamily X} and {\ttfamily Y} are arbitrary types, i.\+e., we observe {\ttfamily bernoulli$<$X-\/$>$Y,K$>$\{f\}} and wish to use that to compute the probability that {\ttfamily f(x) = y} for some {\ttfamily x in X} and {\ttfamily y \textbackslash{}in Y}.

Notice that we do not change the type of the input, {\ttfamily X}. This is a first-\/order Bernoulli map. We can, of course, also provide as input to this function a Bernoulli Boolean value, e.\+g., {\ttfamily bernoulli$<$bool,1$>$\{true\}}, and we will get a an even higher-\/order Bernoulli Boolean value as output. In this case, we willl have a higher-\/order Bernoulli map of type 
\begin{DoxyCode}{0}
\DoxyCodeLine{bernoulli<bool,1> -\/> bernoulli<bool>}

\end{DoxyCode}
 where for the output we drop the order information, and track the error rates using interval arithmetic, whch we will discuss later.

Since functions are values, we can also ask the question, what is the probability that {\ttfamily bernoulli$<$bool-\/$>$bool,1$>$\{id\} == id}? In this case, we are asking about the equality of the functions, which is mathematically equivalent to asking whether each input in the domain maps to the same output, i.\+e., 
\begin{DoxyCode}{0}
\DoxyCodeLine{Pr\{bernoulli<\textcolor{keywordtype}{bool}-\/>bool,1>\{\textcolor{keywordtype}{id}\}\textcolor{keyword}{true}) == \textcolor{keywordtype}{id}(\textcolor{keyword}{true}) \&\&}
\DoxyCodeLine{    bernoulli<\textcolor{keywordtype}{bool}-\/>bool,1>\{\textcolor{keywordtype}{id}\}(\textcolor{keyword}{false}) == \textcolor{keywordtype}{id}(\textcolor{keyword}{false})\}}

\end{DoxyCode}
 Since this is a first-\/order model, the probability that both conditions are true is just the product of the probabilities of each condition being true, i.\+e., 
\begin{DoxyCode}{0}
\DoxyCodeLine{Pr\{bernoulli<\textcolor{keywordtype}{bool}-\/>bool>,1>\{\textcolor{keywordtype}{id}\}(\textcolor{keyword}{true}) == \textcolor{keywordtype}{id}(\textcolor{keyword}{true})\} *}
\DoxyCodeLine{    Pr\{bernoulli<\textcolor{keywordtype}{bool}-\/>bool>,1>\{\textcolor{keywordtype}{id}\}(\textcolor{keyword}{false}) == \textcolor{keywordtype}{id}(\textcolor{keyword}{false})\} = p\string^2.}

\end{DoxyCode}


Let\textquotesingle{}s fix {\ttfamily p} and consider the confusion matrix for the first-\/order model, {\ttfamily bernoulli$<$bool-\/$>$bool,1$>$}. We used the standard naming convention for the outcomes of observations ({\ttfamily bernoulli$<$bool-\/$>$bool,1$>$\{f\}(x)}) when compared against the actuality (the latent {\ttfamily f(x)}), where TPR is the true positive rate, FNR is the false negative rate, TNR is the true negative rate, and FPR is the false positive rate. The confusion matrix is given by Table 2.

Table 2\+: First-\/\+Order Bernoulli Model for {\ttfamily bool -\/$>$ bool} over Booleans

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{3}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ observe {\ttfamily true}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ observe {\ttfamily false}    }\\\cline{1-3}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ observe {\ttfamily true}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ observe {\ttfamily false}    }\\\cline{1-3}
\endhead
latent {\ttfamily true}   &TPR {\ttfamily p}   &FNR {\ttfamily 1-\/p}    \\\cline{1-3}
latent {\ttfamily false}   &FPR {\ttfamily 1-\/p}   &TNR {\ttfamily p}   \\\cline{1-3}
\end{longtabu}


Note that in the above, we are not discussing the input -- it is, after all, observable in this case. We are only discussing the output, which is latent, since we are pretending that we do not know we are dealing with, say, {\ttfamily id}. We are only given the observation {\ttfamily bernoulli$<$bool-\/$>$bool,1$>$\{id\}}. As mentioned previously, there are only 4 possible functions of type {\ttfamily bool -\/$>$ bool}, so if {\ttfamily p} is reasonably small, we can probably estimate the true function with high confidence based on examing inputs with expected outputs.

We might ask the question, can the order {\ttfamily N} in {\ttfamily bernoulli$<$bool-\/$>$bool,N$>$} be greater than 2? It is an interesting question. We only have two possible outcomes, {\ttfamily true} and {\ttfamily false}, so how could we have a higher-\/order model? The answer is that we are not tracking the order of the output, but rather, we are tracking the order of the Bernoulli Boolean {\itshape function} approximation. Since we know the type, {\ttfamily bool -\/$>$ bool}, we know that there are only 4 possible functions.

Just as before, we knew we had a Boolean value. A Boolean value can only be {\ttfamily true} or {\ttfamily false}. We can\textquotesingle{}t observe the value directly, but we can observe a Bernoulli approximation of the value. For each observed value, we can have unique probability that the latent value is {\ttfamily true} or {\ttfamily false}.

Let\textquotesingle{}s extend this to the discussion of functions of type {\ttfamily bool -\/$>$ bool}. There are only 4 possible functions of this type, {\ttfamily id}, {\ttfamily not}, {\ttfamily true}, and {\ttfamily false}.

Now suppose we are given a Bernoulli {\ttfamily bernoulli$<$bool-\/$>$bool$>$\{id\}}. We do not know that the latent function is {\ttfamily id}, we only know that we have a function {\ttfamily bernoulli$<$bool-\/$>$bool$>$\{id\}}, which can be either {\ttfamily id}, {\ttfamily not}, {\ttfamily true}, or {\ttfamily false}. The best guess for {\ttfamily bernoulli$<$bool-\/$>$bool$>$\{id\}} is the function that it matches, assuming that the process that generates these approximations is unbiased.

Let\textquotesingle{}s construct the confusion matrix for {\ttfamily bernoulli$<$bool-\/$>$bool$>$}.

Table 3\+: Bernoulli Model for {\ttfamily bool -\/$>$ bool}

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ latent / observe   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily id}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily not}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily true}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily false}    }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ latent / observe   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily id}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily not}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily true}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily false}    }\\\cline{1-5}
\endhead
{\ttfamily id}   &{\ttfamily p11}   &{\ttfamily p12}   &{\ttfamily p13}   &{\ttfamily p14}    \\\cline{1-5}
{\ttfamily not}   &{\ttfamily p21}   &{\ttfamily p22}   &{\ttfamily p23}   &{\ttfamily p24}    \\\cline{1-5}
{\ttfamily true}   &{\ttfamily p31}   &{\ttfamily p32}   &{\ttfamily p33}   &{\ttfamily p34}    \\\cline{1-5}
{\ttfamily false}   &{\ttfamily p41}   &{\ttfamily p42}   &{\ttfamily p43}   &{\ttfamily p44}   \\\cline{1-5}
\end{longtabu}


Each row must sum to 1, {\ttfamily \textbackslash{}sum\+\_\+j p\+\_\+\{i j\} = 1}, so we only have up to a maximum of {\ttfamily 4 (4-\/1) = 12} degrees of freedom. This means the highest Bernoulli Boolean order is 12 ({\ttfamily bernoulli$<$bool-\/$>$bool,12$>$}), but we normally drop the order and just write {\ttfamily bernoulli$<$bool-\/$>$bool$>$} and track the error rates using interval arithmetic, as mentioned a few times previously.

Now, when we have a Bernoulli approximation of some latent function of type {\ttfamily bool -\/$>$ bool}, we wish to store the error information in the output so that we can propagate it forward. We do this by saying that the output is a Bernoulli Boolean, because it may or may not be correct, i.\+e., the Bernoulli process {\ttfamily bernoulli$<$bool-\/$>$bool$>$} generates a function of type {\ttfamily bool -\/$>$ bernoulli$<$bool$>$} rather than of type {\ttfamily bool -\/$>$ bool}. In our algorithms, we created a type system for this, and this extra information can be discarded when tracking errors is not needed.

So, what happens when we have a Bernoulli model {\ttfamily bernoulli$<$bool-\/$>$bool$>$}, and then we lift it to 
\begin{DoxyCode}{0}
\DoxyCodeLine{bernoulli<bernoulli<bool>-\/>bernoulli<\textcolor{keywordtype}{bool}>>}

\end{DoxyCode}
 by providing {\ttfamily bernoulli$<$bool$>$} as input? When we compare the true output with this lifted Bernoulli model, we still get a maximum order of 12, but if the order is, say, 2, then this lifted model is likely to have a higher order.

The order of the model is not necessarily that important, but it does complicate estimation problems, and it is also {\itshape desirable} to have a higher order models in some cases, for instance if we have an entropy coder, then we want the diagonal of the confusion matrix to be as close to 1 as possible, and we want the off-\/diagonal elements to be as close to 0 as possible, but when elements are not 0, we want functions that are more similiar to the latent function to have larger probabilities than functions that are less similiar to the latent function. This is just a way of minimizing a loss function in ML, where the function truly is latent and we are trying to find the best approximation to the latent function by minimizing a loss function. The higher the order, the more capacity the model has to approximate the latent function, but the more data we need to estimate the parameters of the model.

ML is not really the target of the Bernoulli model, but it is a useful way to think about the model. The Bernoulli model is really a way of thinking about the uncertainty in the output of a function, and how that uncertainty propagates through a computation, and typically the uncertainty is due to a trade-\/off between space complexity and accuracy. The more space we use to represent the function, the more closely it is expected to approximate the latent function.\hypertarget{md_BERNOULLI_BOOL_autotoc_md6}{}\doxysection{Noisy Turing machines\+: noisy logic gates}\label{md_BERNOULLI_BOOL_autotoc_md6}
As we consider more complex compound data types, which may always be modeled as functions, we will see that there are many ways these types can participate in the Bernoulli Boolean model. When a Bernoulli value is introduced into the computational model, the entire computation outputs a final result that is a Bernoulli type, e.\+g., {\ttfamily bernoulli$<$pair$<$T1,T2$>$$>$}, {\ttfamily pair$<$T1,bernoulli$<$T2$>$}, and so on.

The easiest way to think about this is to just consider a Universal Turing machine in which we build programs by composing circuits of binary logic-\/gates, like {\ttfamily and}, {\ttfamily or}, and {\ttfamily not}. In general, if we replace a single input into the circuit with a Bernoulli Boolean, the output of the circuit is a one or more Bernoulli Booleans. Moreover, and more interestingly, we can replace some of the logic gates with noisy logic-\/gates, or Bernoulli logic-\/gates, and the output of the circuit is also a Bernoulli Boolean. We can always discard information about the uncertainty in the output of the circuit, and just get Boolean, but if the uncertainty is non-\/negligible, then we may want to keep track of it.

So, let\textquotesingle{}s consider the set of binary functions {\ttfamily f \+: (bool, bool) -\/$>$ bool}.

There are 2$^\wedge$2 = 4 possible functions {\ttfamily f \+: bool -\/$>$ bool} since for each possible input, {\ttfamily true} or {\ttfamily false}, we have two possible outputs, {\ttfamily true} or {\ttfamily false}.

\begin{quote}
More generally, if we have {\ttfamily f \+: X -\/$>$ Y}, then we have {\ttfamily $\vert$\+Y$\vert$$^\wedge$$\vert$\+X$\vert$} possible functions, where {\ttfamily $\vert$.$\vert$} denotes the cardinality of a set. For instance, if {\ttfamily X = (bool, bool)} and {\ttfamily Y = bool}, then we have {\ttfamily 2$^\wedge$4 = 16} possible functions, since {\ttfamily $\vert$\+X$\vert$ = 4} and {\ttfamily $\vert$\+Y$\vert$ = 2}. \end{quote}
Each of these functions has a designated name, which we can use to refer to them, like {\ttfamily and}, {\ttfamily xor}, etc. However, we are just going to look at {\ttfamily and}.

Table 4\+: {\ttfamily and \+: (bool, bool) -\/$>$ bool}

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{3}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x1}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x2}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily and(x1, x2)}    }\\\cline{1-3}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x1}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x2}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily and(x1, x2)}    }\\\cline{1-3}
\endhead
true   &true   &true    \\\cline{1-3}
true   &false   &false    \\\cline{1-3}
false   &true   &false    \\\cline{1-3}
false   &false   &false   \\\cline{1-3}
\end{longtabu}


Now, let\textquotesingle{}s consider 
\begin{DoxyCode}{0}
\DoxyCodeLine{and : (bernoulli<bool,1>, bernoulli<bool,1>) -\/> bernoulli<bool,2>`}

\end{DoxyCode}


This is more complicated than might first seem. An error occurs if {\ttfamily and} returns {\ttfamily true} when it should return {\ttfamily false}, or vice versa. The input variables represent {\itshape latent} values, so they do not have a definite value.

We will go row by row, and examine the probability that the output is correct for each {\itshape output}.\hypertarget{md_BERNOULLI_BOOL_autotoc_md7}{}\doxysubsection{Case 1\+: The Correct Output Is True}\label{md_BERNOULLI_BOOL_autotoc_md7}
In order for the output to be true, both noisy inputs must be true, which is just the product of the probabilities of each condition being true since they are statistically independent outcomes.\hypertarget{md_BERNOULLI_BOOL_autotoc_md8}{}\doxysubsection{Case 2\+: The Correct Output Is False Given $<$tt$>$x1 = true$<$/tt$>$ and $<$tt$>$x2 = false$<$/tt$>$}\label{md_BERNOULLI_BOOL_autotoc_md8}
Consider {\ttfamily and(bernoulli$<$bool,1$>$\{true\}, bernoulli$<$bool,1$>$\{false\})}. For this to be true, the first must be a true positive and the second must be a false postive, which is just {\ttfamily p1 $\ast$ (1-\/p2)}. Since we are interested in the probability that it correctly maps to false, that is just {\ttfamily 1 -\/ p1 $\ast$ (1-\/p2) = 1 -\/ p1 + p1 $\ast$ p2}.\hypertarget{md_BERNOULLI_BOOL_autotoc_md9}{}\doxysubsection{Case 3\+: The Correct Output Is False Given $<$tt$>$x1 = false$<$/tt$>$ and $<$tt$>$x2 = true$<$/tt$>$}\label{md_BERNOULLI_BOOL_autotoc_md9}
Consider {\ttfamily and(bernoulli$<$bool,1$>$\{false\}, bernoulli$<$bool,1$>$\{true\})}. For this to be true, the first must be a false positive and the second must be a true positive, which is just {\ttfamily (1-\/p1) $\ast$ p2}. Since we are interested in the probability that it maps correctly to false, that is just {\ttfamily 1 -\/ (1-\/p1) $\ast$ p2 = 1 -\/ p2 + p1 $\ast$ p2}.\hypertarget{md_BERNOULLI_BOOL_autotoc_md10}{}\doxysubsection{Case 4\+: The Correct Output Is False Given $<$tt$>$x1 = false$<$/tt$>$ and $<$tt$>$x2 = false$<$/tt$>$}\label{md_BERNOULLI_BOOL_autotoc_md10}
Consider {\ttfamily and(bernoulli$<$bool,1$>$\{false\}, bernoulli$<$bool,1$>$\{false\})}. For this to be true, both must be false positives, which is just {\ttfamily (1-\/p1) $\ast$ (1-\/p2)}. Since we are interestd in the probability that it maps correctly to false, that is just {\ttfamily 1 -\/ (1-\/p1) $\ast$ (1-\/p2) = p1 + p2 -\/ p1 $\ast$ p2}.\hypertarget{md_BERNOULLI_BOOL_autotoc_md11}{}\doxysection{Summary}\label{md_BERNOULLI_BOOL_autotoc_md11}
Table 6\+: {\ttfamily and} with Bernoulli inputs

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{4}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x1}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x2}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily and(x1,x2)}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily Pr\{correct\}}    }\\\cline{1-4}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x1}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily x2}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily and(x1,x2)}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily Pr\{correct\}}    }\\\cline{1-4}
\endhead
1   &1   &1   &{\ttfamily p1 $\ast$ p2}    \\\cline{1-4}
1   &0   &0   &{\ttfamily 1 -\/ p1 + p1 $\ast$ p2}    \\\cline{1-4}
0   &1   &0   &{\ttfamily 1 -\/ p2 + p1 $\ast$ p2}    \\\cline{1-4}
0   &0   &0   &{\ttfamily p1 + p2 -\/ p1 $\ast$ p2}   \\\cline{1-4}
\end{longtabu}


We see that {\ttfamily and \+: (bernoulli$<$bool,1$>$, bernoulli$<$bool,1$>$) -\/$>$ bernoulli$<$bool,4$>$} induces an output that is a fourth-\/order Bernoulli Boolean. How is this possible when there are only two possible outputs? The answer is that the output is dependent on four different combinations of inputs.

Since {\ttfamily x1} and {\ttfamily x2} are {\itshape latent}, we can only talk about the probability that the output is correct or not. We see that when the output is 1, the probability that the output is correct is {\ttfamily p1 $\ast$ p2}. When the output is 0, the probability that it is correct is more complicated.

We could store all of this information in the type {\ttfamily bernoulli$<$bool,4$>$}, but it is probably more convenient to use interval arithmetic, where we store a range of probabilities for the probabily that the Boolean value being stored is correct. The best choice is just the minimum length interval that contains all of the relevant probabilities for the output being correct. When the output is 1, we see that the minimum spanning interval is just {\ttfamily p1 $\ast$ p2}, and when the output is 0, the minimum spanning interval is just the minimum span of 
\begin{DoxyCode}{0}
\DoxyCodeLine{min\_span\{1 -\/ p1 + p1 * p2, 1 -\/ p2 + p1 * p2, p1 + p2 -\/ p1 * p2\}}

\end{DoxyCode}


As we compose more and more logic circuits together, we can keep track of the minimum spanning intervals on outputs using interval arithmetic.

Let\textquotesingle{}s come back to the idea of Bernoulli types over compound types. In particular, let\textquotesingle{}s consider applynig the Bernoulli approximation to binary functions of the type {\ttfamily (bool, bool) -\/$>$ bool}.

Now, we can apply the Bernoulli approximation 
\begin{DoxyCode}{0}
\DoxyCodeLine{bernoulli<(bool, bool) -\/> \textcolor{keywordtype}{bool}>}

\end{DoxyCode}
 which will generate functions of the type 
\begin{DoxyCode}{0}
\DoxyCodeLine{(bool, bool) -\/> bernoulli<bool>}

\end{DoxyCode}


This may be thought of as a {\itshape noisy} binary logic-\/gate. For the case of the {\ttfamily and} gate, what we observe in our model is {\ttfamily bernoulli$<$(bool, bool) -\/$>$ bool$>$\{and\}}, and it can generate up to 16 different Bernoulli Boolean functions. That means that the maximum order is {\ttfamily 16 (16 -\/ 1) = 240}, which isn\textquotesingle{}t really important, but it\textquotesingle{}s interesting to note.

Of course, if we have this noisy {\ttfamily and} function and then put in noisy inputs, then we get a function of type 
\begin{DoxyCode}{0}
\DoxyCodeLine{(bernoulli<bool>, bernoulli<bool>) -\/> bernoulli<bool>}

\end{DoxyCode}
 