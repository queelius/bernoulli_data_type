The bernoulli map quantifies a certain kind of approximation error that is compatible with many algorithms, particularly data types that depend on hashing.

Suppose we have a {\itshape latent} function {\ttfamily p \+: X -\/$>$ Y} and an observale approximation or noisy version of {\ttfamily p}, denoted by {\ttfamily p$\ast$ \+: X -\/$>$ Y}, such that {\ttfamily p$\ast$(x) != p(x)} with probability {\ttfamily e(x)}, where {\ttfamily \{e(x) \+: x in X\}} are apriori statistically independent random variables.

When we observe {\ttfamily p$\ast$}, we know that it is a noisy model of some function {\ttfamily X -\/$>$ Y}, but we do not know which one with certainty. If we are given no other prior information, then the best estimator of {\ttfamily p} is {\ttfamily p$\ast$}.

Let the functions of type {\ttfamily X -\/$>$ Y} be named {\ttfamily p1, p2, ..., pn}, where {\ttfamily n} is the total number of functions in {\ttfamily X -\/$>$ Y}, which if no other information is given, is given by the cardinality of {\ttfamily X -\/$>$ Y}, which is just {\ttfamily n = $\vert$\+Y$\vert$$^\wedge$$\vert$\+X$\vert$}.

Let us show the confusion matrix for {\ttfamily p1, p2, p3, p4} ({\ttfamily n=4}, e.\+g., {\ttfamily bool -\/$>$ bool}), where the rows represent the latent functions, and the columns represent the observed functions that may be generated from the latent function, say by introducing some error or rate distortion.

\tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ latent \textbackslash{} observed   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p1}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p2}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p3}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p4}    }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ latent \textbackslash{} observed   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p1}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p2}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p3}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ {\ttfamily p4}    }\\\cline{1-5}
\endhead
{\ttfamily p1}   &{\ttfamily q11}   &{\ttfamily q12}   &{\ttfamily q13}   &{\ttfamily q14}    \\\cline{1-5}
{\ttfamily p2}   &{\ttfamily q21}   &{\ttfamily q22}   &{\ttfamily q23}   &{\ttfamily q24}    \\\cline{1-5}
{\ttfamily p3}   &{\ttfamily q31}   &{\ttfamily q32}   &{\ttfamily q33}   &{\ttfamily q34}    \\\cline{1-5}
{\ttfamily p4}   &{\ttfamily q41}   &{\ttfamily q42}   &{\ttfamily q43}   &{\ttfamily q44}   \\\cline{1-5}
\end{longtabu}


In the matrix above, {\ttfamily qij} represents the probability that function {\ttfamily pi} is observed as function {\ttfamily pj}. Since each row must sum to 1, there are at maximum of {\ttfamily n $\ast$ (n-\/1)} degrees of freedom, which in this case is {\ttfamily 4 $\ast$ (4-\/1) = 12}.

We call this degree-\/of-\/freedom the {\itshape order} of the Bernoulli Model for {\ttfamily X -\/$>$ Y}. In many cases, the order is 1, e.\+g., where most of the probability is assigned to the diagonal, and the off-\/diagonal elements are nearly zero (or zero) but all equal.

Since we are only given {\ttfamily p$\ast$}, we do not know which {\ttfamily p} is the true latent function. We say that {\ttfamily p$\ast$} is a {\itshape Bernoulli approximation} of {\ttfamily p}, and we deote this by writing {\ttfamily p$\ast$ $\sim$ bernoulli$<$X-\/$>$Y$>$\{p\}}, i.\+e., we observe {\ttfamily p$\ast$} but we know that it is a random observation from a conditional distribution where we are conditioning on the latent function {\ttfamily p} to generate {\ttfamily p$\ast$}.

If we observe a realization of {\ttfamily p$\ast$ $\sim$ bernoulli$<$X-\/$>$Y$>$\{p\}}, where {\ttfamily p} is latent, and we know that it is a Bernoulli approximation, then we say that {\ttfamily p$\ast$} is of the type 
\begin{DoxyCode}{0}
\DoxyCodeLine{X -\/> bernoulli<Y>}

\end{DoxyCode}


Normally the order of the Bernoulli Model is not that important, but it may be, e.\+g., if we are trying to estimate the latent function {\ttfamily p} from {\ttfamily p$\ast$}, and we know that the order is 1, then we can estimate the confusion matrix more easily given an i.\+i.\+d. sample of observations.

A more {\itshape interesting} property, that can be read off the confusion matrix, is the entropy of the distribution {\ttfamily bernoulli$<$X-\/$>$Y$>$}. This is given by 
\begin{DoxyCode}{0}
\DoxyCodeLine{H(bernoulli<X-\/>Y>) = -\/sum\_\{i=1\}\string^n sum\_\{j=1\}\string^n qij log(qij)}

\end{DoxyCode}
 where {\ttfamily qij} is the probability that {\ttfamily pi} is observed as {\ttfamily pj}.

We can also consider the conditional entropy distribution, {\ttfamily bernoulli$<$X-\/$>$Y$\vert$pi$>$}, where {\ttfamily pi} is the latent function. This is given by 
\begin{DoxyCode}{0}
\DoxyCodeLine{H(bernoulli<X-\/>Y|pi>) = -\/sum\_\{j=1\}\string^n qij log(qij)}

\end{DoxyCode}
 where {\ttfamily qij} is the probability that {\ttfamily pi} is observed as {\ttfamily pj}.

Often, we apriori {\itshape know} the confusion matrix, or at least various properties of this matrix, as a result of the distortion being the result of some known process, e.\+g., a noisy channel, or a noisy sensor, or a noisy measurement. A noisy channel also includes things like a program that introduces some loss as a way of compressing the data, or it may be the result of some homomorphic encryption scheme or a homomophorism for an oblivious data structure where we represent values as the product of trapdoors, one-\/way hashes.\hypertarget{md_BERNOULLI_MAP_autotoc_md13}{}\doxysection{Bernoulli Maps}\label{md_BERNOULLI_MAP_autotoc_md13}
A Bernoulli Map is just a way of generating a Bernoulli approximation of a function. By the equivalence that data is code and code is data, any function can be represented as a data structure, and any data structure can be represented as a function. Therefore, theoretically, we can model any data structure as a map, and then we can generate a Bernoulli approximation of that map, which means we have a Bernoulli approximation of the data structure.

Often, we have more efficient and interesting ways to generate particular kinds of Bernoulli approximations of data structures. Probably, the most popular example are sets, like Bloom filters.\hypertarget{md_BERNOULLI_MAP_autotoc_md14}{}\doxysubsection{Set-\/indicator function $<$tt$>$1\+\_\+\+A \+: X -\/$>$ bool$<$/tt$>$}\label{md_BERNOULLI_MAP_autotoc_md14}
The set-\/indicator function for {\ttfamily A}, denoted by {\ttfamily 1\+\_\+A}, where {\ttfamily 1\+\_\+\+A(x)} is {\ttfamily true} if {\ttfamily x} is in {\ttfamily A}, and {\ttfamily false} otherwise. When we apply a Bernoulli Model to {\ttfamily 1\+\_\+A}, we get a function of type {\ttfamily X -\/$>$ bernoulli$<$bool$>$}. We observe {\ttfamily bernoulli$<$X-\/$>$bool$>$\{1\+\_\+A\}} but we do not know {\ttfamily A} with certainty. This is a {\itshape Bernoulli approximation} of {\ttfamily 1\+\_\+A}, and common examples of this kind of approximation are {\itshape bloom filters} and {\itshape counting bloom filters}. In this project, we introduce the Bernoulli Map, which is an algorithm that can generate any kind of approximation of computable functions, including set-\/indicator functions.\hypertarget{md_BERNOULLI_MAP_autotoc_md15}{}\doxysubsection{Primality test\+: $<$tt$>$is\+\_\+prime \+: integer -\/$>$ bernoulli$<$bool$>$$<$/tt$>$}\label{md_BERNOULLI_MAP_autotoc_md15}
We know how to exactly determine whether an integer is prime. We can, for instance, check for divisibility by all integers less than the integer. There are many ways to more efficiently compute this, but the point is that we know how to compute it exactly.

However, the function is still {\itshape latent} in the sense that the time required to compute it exactly for any input of interest is prohibitive, and so in practice we do not know its extension. It is still, in this sense, latent.

So, instead, we can use a randomized algorithm to estimate the function and be able to compute it for any desired input in a reasonable amount of time.\hypertarget{md_BERNOULLI_MAP_autotoc_md16}{}\doxysubsubsection{The Miller-\/\+Rabin primality test}\label{md_BERNOULLI_MAP_autotoc_md16}
The Miller-\/\+Rabin primality test is based on the concept of Fermat\textquotesingle{}s Little Theorem, which states that if {\ttfamily p} is a prime number and {\ttfamily a} is any positive integer less than {\ttfamily p}, then {\ttfamily a$^\wedge$(p-\/1)} is congruent to 1 modulo {\ttfamily p}.

The Miller-\/\+Rabin test works by randomly selecting values of {\ttfamily a} and checking whether the congruence holds. If the congruence fails for a particular {\ttfamily a}, then {\ttfamily p} is definitely not prime. However, if the congruence holds for some {\ttfamily a}, then {\ttfamily p} may or may not be prime but we say that it is prime, which has some specifiable probability of error (false positive rate).

In essence, a particular seed value (for the PRNG) draws a sample function, a Bernoulli map, from {\ttfamily is\+\_\+prime$\ast$ $\sim$ bernoulli$<$integer -\/$>$ bool$>$\{is\+\_\+prime\}}.\hypertarget{md_BERNOULLI_MAP_autotoc_md17}{}\doxysection{Computational basis}\label{md_BERNOULLI_MAP_autotoc_md17}
If we have a set of functions {\ttfamily F = \{ f1, ..., fk \}}, then we can define a Bernoulli model over {\ttfamily F} by simply generating realizations of {\ttfamily bernoulli\{f1\}, ... bernoulli\{fk\}} which we may denote as {\ttfamily bernoulli\{F\}}.

For instance, it may be desirable to support both {\ttfamily in} and {\ttfamily ==} for sets. One approach is to generate a Bernoulli aproximation for each element in {\ttfamily F}. However, if we define {\ttfamily ==} in terms of {\ttfamily in}, then that {\itshape induces} a Bernoulli Model of {\ttfamily ==} through its dependence on {\ttfamily in}.\hypertarget{md_BERNOULLI_MAP_autotoc_md18}{}\doxysection{Regular types}\label{md_BERNOULLI_MAP_autotoc_md18}
It is interesting to note that Bernoulli Models are not in general regular types, since it is often the case that, say, a Bernoulli set {\ttfamily A} can have countably infinite representations, and it is impossible (in general) to determine if two Bernoulli sets are the same.

This does not even entertain the discussion about which latent set is being approximately modeled by a Bernoulli set, which can of course also vary. If we consider this perspective, then equality on Bernoulli sets vs sets is not of type 
\begin{DoxyCode}{0}
\DoxyCodeLine{(bernoulli<set>, set) -\/> bool}

\end{DoxyCode}
 but of type 
\begin{DoxyCode}{0}
\DoxyCodeLine{(bernoulli<set>, set) -\/> bernoulli<bool>}

\end{DoxyCode}
 and likewise for other variations on this pattern, i.\+e., we can only say what the probability that a Bernoulli set represetns a given latent set. This is normally a much less interesting and informative question than set-\/membership, but it is still a question that can be asked. 